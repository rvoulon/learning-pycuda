{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Even Easier Introduction to CUDA\n",
    "https://devblogs.nvidia.com/even-easier-introduction-cuda/  \n",
    "Adapted for PyCUDA by [Roberta Voulon](https://github.com/rvoulon).  \n",
    "I'm just learning this stuff as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure everything is working as expected\n",
    "print(f\"{cuda.Device.count()} device(s) found\")\n",
    "for i in range(cuda.Device.count()):\n",
    "    dev = cuda.Device(i)\n",
    "    print(f\"Device {i}: {dev.name()}\")\n",
    "    a, b = dev.compute_capability()\n",
    "    print(f\"  Compute capability: {a}.{b}\")\n",
    "    print(f\"  Total memory: {dev.total_memory() / 1024} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting from a regular loop\n",
    "https://devblogs.nvidia.com/even-easier-introduction-cuda/  \n",
    "\n",
    "Here's the starting code in C++, we'll need to port that to Python:\n",
    "\n",
    "\n",
    "```c++\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "// function to add the elements of two arrays\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20; // 1M elements\n",
    "\n",
    "  float *x = new float[N];\n",
    "  float *y = new float[N];\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the CPU\n",
    "  add(N, x, y);\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  delete [] x;\n",
    "  delete [] y;\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's that same code in Python:\n",
    "(Yes I know it's not super pythonic, don't worry about that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(n, x, y):\n",
    "    \"\"\"Function to add the elements of two arrays\"\"\"\n",
    "    for i in range(0, n):\n",
    "        y[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1<<20 # 1M elements\n",
    "x = np.ones(N, dtype=np.float32)\n",
    "y = np.full(N, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "add(N, x, y)\n",
    "print(f\"---- {time.time() - start} seconds ----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "max_error = 0.0\n",
    "for i in range(0, N):\n",
    "    global max_error\n",
    "    max_error = max(max_error, (y[i] - 3.0))\n",
    "print(f\"Max error: {max_error}\")\n",
    "print(f\"---- {time.time() - start} seconds ----\")\n",
    "\n",
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single thread on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get this running on the GPU\n",
    "1. set up your data (array/vector, matrix, ...) on the host, setting type to `np.float32`\n",
    "1. allocate space on the GPU's memory and copy the data to it (to device)\n",
    "1. write the key computational kernel in c for the GPU\n",
    "1. get the function and call it, give as parameters the pointer(s) to your data on the GPU and the block size (making sure the data and blocksize have the same number of dimensions, 1D, 2D or 3D)\n",
    "1. synchronize with the device: wait for GPU to finish before accessing on host\n",
    "1. create a new variable to contain the data from the GPU and copy it (to host)\n",
    "1. free up the memory on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1<<20 # 1M elements\n",
    "x = np.ones(N, dtype=np.float32)\n",
    "y = np.full(N, 2).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cuda.mem_alloc(x.nbytes)\n",
    "y_gpu = cuda.mem_alloc(y.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_htod(x_gpu, x)\n",
    "cuda.memcpy_htod(y_gpu, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop won't be here for long, just stay with me\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void add(float *x, float *y)\n",
    "    {\n",
    "        int n = 1<<20;\n",
    "        for (int i = 0; i < n; i++)\n",
    "            y[i] = x[i] + y[i];\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "func = mod.get_function(\"add\")\n",
    "# We're just using a single thread for now\n",
    "func(x_gpu, y_gpu, block=(1, 1, 1))\n",
    "y_added = np.empty_like(y)\n",
    "cuda.memcpy_dtoh(y_added, y_gpu)\n",
    "print(f\"---- {time.time() - start} seconds ----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu.free()\n",
    "y_gpu.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "max_error = 0.0\n",
    "for i in range(0, N):\n",
    "    global max_error\n",
    "    max_error = max(max_error, (y_added[i] - 3.0))\n",
    "print(f\"Max error: {max_error}\")\n",
    "print(f\"---- {time.time() - start} seconds ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZOMG! It worked! ðŸ™€ðŸ™€ðŸ™€\n",
    "Even a single thread on the GPU is already much faster! Alright, now let's use parallel threading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same example but parallelly on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1<<20 # 1M elements\n",
    "x = np.ones(N, dtype=np.float32)\n",
    "y = np.full(N, 2).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cuda.mem_alloc(x.nbytes)\n",
    "y_gpu = cuda.mem_alloc(y.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_htod(x_gpu, x)\n",
    "cuda.memcpy_htod(y_gpu, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix that loop and make it parallellizable\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void add(float *x, float *y)\n",
    "    {\n",
    "        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        y[idx] = x[idx] + y[idx];\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "func = mod.get_function(\"add\")\n",
    "## Now using a block of 256 x 1 x 1 threads (1D)\n",
    "func(x_gpu, y_gpu, block=(256, 1, 1))\n",
    "y_added = np.empty_like(y)\n",
    "cuda.memcpy_dtoh(y_added, y_gpu)\n",
    "print(f\"---- {time.time() - start} seconds ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘†ðŸ‘†ðŸ‘† Holy sh\\*t, batman!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu.free()\n",
    "y_gpu.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "max_error = 0.0\n",
    "for i in range(0, N):\n",
    "    global max_error\n",
    "    max_error = max(max_error, (y_added[i] - 3.0))\n",
    "print(f\"Max error: {max_error}\")\n",
    "print(f\"---- {time.time() - start} seconds ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up?\n",
    "- learn to use the pycuda profiler rather than using `start = time.time()`\n",
    "- learn to use pycuda's `gpuarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
